{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PREPROCESSING",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiobm3/ESI_MachineLearning/blob/NLP/PREPROCESSING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIKabJAeqFMt"
      },
      "source": [
        "\n",
        "\n",
        "> We are the SSJ team and this is our work about natural language processing.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmNVisjhv2oV"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# Libraries for natural language processing\n",
        "import nltk\n",
        "\n",
        "# Libraries for tweets\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Libraries for correct wrong words\n",
        "from collections import Counter\n",
        "\n",
        "# Libraries for emoticons\n",
        "!pip install emoji\n",
        "from emoji import UNICODE_EMOJI\n",
        "\n",
        "# Libraries for stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Libraries for lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFQiyz05K4M0"
      },
      "source": [
        "#For tweets\n",
        "nltk.download('punkt')\n",
        "\n",
        "# For stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# For lemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-uS5d_dJZE"
      },
      "source": [
        "## 0. Loading Data\r\n",
        "\r\n",
        "We start by loading the data file that contains the tweets with which the study will work. The file format to upload is CSV.\r\n",
        "\r\n",
        "For this purpose, we use some commands provided by Google Colab itself to access files on our computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "s7muMGF_CQve",
        "outputId": "80147516-d176-4686-8436-e0fbd6f52b05"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')),sep=',')\n",
        "\n",
        "df"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-66bbda7b-8621-46bb-a488-0e8bd3b1994b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-66bbda7b-8621-46bb-a488-0e8bd3b1994b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving labeled_data.csv to labeled_data (1).csv\n",
            "User uploaded file \"labeled_data.csv\" with length 2546446 bytes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>25291</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>25292</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>25294</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>25295</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>youu got wild bitches tellin you lies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>25296</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows √ó 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                              tweet\n",
              "0               0  ...  !!! RT @mayasolovely: As a woman you shouldn't...\n",
              "1               1  ...  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
              "2               2  ...  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
              "3               3  ...  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
              "4               4  ...  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
              "...           ...  ...                                                ...\n",
              "24778       25291  ...  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
              "24779       25292  ...  you've gone and broke the wrong heart baby, an...\n",
              "24780       25294  ...  young buck wanna eat!!.. dat nigguh like I ain...\n",
              "24781       25295  ...              youu got wild bitches tellin you lies\n",
              "24782       25296  ...  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
              "\n",
              "[24783 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUYtzK1fykzw"
      },
      "source": [
        "3 classes: 0 --> hate speech, 1 --> offensive language, 2 --> neither"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkiEwhzbelrP"
      },
      "source": [
        "# 1. Preprocessing\r\n",
        "\r\n",
        "Before starting, it is necessary to preprocess the tweet field of the dataset that is provided to us, in order to work in a more efficient and accurate way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upQTMV99GXCZ"
      },
      "source": [
        "## Remove unuseful data\r\n",
        "\r\n",
        "Some symbols that do not provide information have been eliminated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojY36_VFHlH3"
      },
      "source": [
        "def cleanUnusefulData(sentence):\n",
        "  sentence = sentence.translate(str.maketrans('', '', pattern))\n",
        "  return sentence\n",
        "\n",
        "def removeUnusefulExclamation(sentence):\n",
        "  if sentence[0] == '!':\n",
        "    sentence = removeUnusefulExclamation(sentence[1:])\n",
        "  else:\n",
        "    return sentence\n",
        "  return sentence\n",
        "\n",
        "clean_tweet = []\n",
        "for t in range(0,len(df)):\n",
        "  tweet = df.iloc[t]['tweet']\n",
        "  tweet = removeUnusefulExclamation(tweet)\n",
        "  clean_tweet.append(tweet)\n",
        "\n",
        "df['clean_tweet'] = clean_tweet\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJJYq7h4au0L"
      },
      "source": [
        "## Replace emoticons\r\n",
        "\r\n",
        "We replace the emoticons with words that resemble what they want to express. You must upload the emoticons.txt file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbx33glitktw",
        "outputId": "eaa38d92-0d47-4c92-ce32-a40c4cf985f0"
      },
      "source": [
        "dict_emoticons = {}\n",
        "with open('emoticons.txt') as f:\n",
        "    for linea in f:\n",
        "      info = linea.split(\",\")\n",
        "      emoticon_code = info[0]\n",
        "      emoticon_word = info[2]\n",
        "      dict_emoticons[emoticon_code] = emoticon_word[:len(emoticon_word)-1]\n",
        "print(dict_emoticons)\n",
        "\n",
        "def replaceEmoticon(word):\n",
        "  if word in dict_emoticons:\n",
        "    word = word.replace(str(word),dict_emoticons[word])\n",
        "  return word\n",
        "\n",
        "def replaceUnknownEmoticon(word):\n",
        "  m = re.match(r\"\\\\[u][A-Za-z0-9]*\",word)\n",
        "  if m is not None:\n",
        "    word = \"\"\n",
        "  return word"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'üòÄ': 'smile', 'üòÅ': 'smile', 'üòÇ': 'laugh', 'üòÉ': 'smile', 'üòÑ': 'smile', 'üòÜ': 'smile', 'üòâ': 'wink', 'üòä': 'smile', 'üòí': 'unamused', 'üòï': 'confused', 'üòó': 'kiss', 'üòò': 'kiss', 'üòô': 'kiss', 'üòö': 'kiss', 'üòû': 'dissapointed', 'üòü': 'worried', 'üò†': 'angry', 'üò°': 'angry', 'üò¢': 'sad', 'üò®': 'frightened', 'üò™': 'sleepy', 'üò´': 'tired', 'üò≠': 'sad', 'üòÆ': 'surprised', 'üòØ': 'surprised', 'üò±': 'frightened', 'üò≤': 'astonished', 'üò≥': 'flushed', 'üò¥': 'sleepy', 'üòµ': 'confused', 'üò∂': 'quiet', 'ü§ê': 'quiet', 'ü§í': 'ill', 'ü§î': 'thoughtful', 'ü§°': 'clown', 'ü§¢': 'sucks', 'ü§£': 'laugh', '\\U0001f92c': 'angry', '\\U0001f92e': 'sucks', '\\U0001f92f': 'astonished', 'üçë': 'ass', 'üçí': 'tits', 'üçå': 'dick', 'üçÜ': 'dick', 'üëâ': 'finger', 'üëå': 'ok'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzmvKruZiOc"
      },
      "source": [
        "## Remove contractions\r\n",
        " \r\n",
        "A dictionary containing the most common contractions has been implemented, to return instead the \"expanded form\" of expressing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG2J7JPHZqIO"
      },
      "source": [
        "dict_contractions = {'aren\\'t':'are not', 'can\\'t':'can not', 'couldn\\'t':'could not', 'didn\\'t':'did not', 'don\\'t':'do not', 'doesn\\'t':'does not', 'hadn\\'t':'had not',\n",
        "                       'haven\\'t':'have not', 'he\\'s':'he is', 'he\\'ll':'he will', 'he\\'d':'he would', 'here\\'s':'here is', 'i\\'m':'i am', 'i\\'ve':'i have', 'i\\'ll':'i will',\n",
        "                       'i\\'d':'i would', 'isn\\'t':'is not','it\\'s':'it is', 'it\\'ll':'it will', 'mustn\\'t':'must not', 'she\\'s':'she is', 'she\\'ll':'she will', 'she\\'d':'she would',\n",
        "                       'shouldn\\'t':'should not', 'that\\'s':'that is', 'there\\'s':'there is', 'they\\'re':'they are', 'they\\'ve':'they have', 'they\\'ll':'they will', 'they\\'d':'they would',\n",
        "                       'they\\'d':'they had', 'wasn\\'t':'was not', 'we\\'re':'we are', 'we\\'re':'we are', 'we\\'ve':'we have', 'we\\'ll':'we will', 'we\\'d':'we would', 'weren\\'t':'were not', 'what\\'s':'what is',\n",
        "                       'where\\'s':'where is', 'who\\'s':'who is', 'who\\'ll':'who will', 'won\\'t':'will not', 'wouldn\\'t':'would not', 'you\\'re': 'you are', 'you\\'ve': 'you have', \n",
        "                       'you\\'ll':'you will', 'you\\'d':'you would', 'y\\'all': 'you all', 'could\\'ve': 'could have', 'hasn\\'t': 'has not', 'let\\'s': 'let us'}\n",
        "\n",
        "def remove_contractions(word):\n",
        "  return word.replace(word,dict_contractions[word])\n",
        "  \n",
        "def is_remove_contractions(word):\n",
        "  if word in dict_contractions:\n",
        "    return True    \n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBPUWLdKSckv"
      },
      "source": [
        "## Executing preprocessing\r\n",
        "\r\n",
        "All the functions defined above will be executed. And other preprocessing steps that have not needed to be defined in a separate function, such as deleting all capital letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNDh43SCZeG2"
      },
      "source": [
        "# We create a new dataframe to save result in different columns\n",
        "df_result = pd.DataFrame()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "U7cE8FWj3SQX",
        "outputId": "f2af3a0e-e03e-4e25-9b6b-4bdae8852e82"
      },
      "source": [
        "pattern = \"\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~‚Äú‚Äù‚Ä¶¬ª‚Äô!\"\n",
        "p.set_options(p.OPT.EMOJI, p.OPT.URL,p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.SMILEY, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED, p.OPT.NUMBER)\n",
        "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "\n",
        "###### EXECUTING PREPROCESSING WITHOUT EMOTICONS ######\n",
        "\n",
        "# Each sentence of tweet\n",
        "sentences = []\n",
        "for tweet in df['clean_tweet']:\n",
        "    result = tknzr.tokenize(tweet)\n",
        "    list_token = []\n",
        "    for word in result:\n",
        "      # Change emoticon for text\n",
        "      token = replaceEmoticon(word)\n",
        "      token = replaceUnknownEmoticon(token.encode('unicode-escape').decode('ASCII'))\n",
        "\n",
        "      if token == \"\": \n",
        "        pass\n",
        "      else: token = token.encode('ASCII').decode('unicode-escape')\n",
        "\n",
        "      # Clean hashtag and mentions\n",
        "      token = p.clean(token)\n",
        "\n",
        "      # Remove the capital letters, if the word is not capitalized entirely.\n",
        "      token = token.lower()\n",
        "\n",
        "      if is_remove_contractions(token): # Remove contractions\n",
        "        token = remove_contractions(token)\n",
        "        token = token.split(\" \")\n",
        "        list_token.append(token[0])\n",
        "        list_token.append(token[1])\n",
        "      else: # Clean unuseful data (\",_,...)\n",
        "        token = cleanUnusefulData(token)\n",
        "        if(token is not \"\"):\n",
        "          list_token.append(token)\n",
        "      \n",
        "    sentences.append(list_token)\n",
        "\n",
        "df_result['preprocessing_without_emoticons'] = sentences\n",
        "df_result"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessing_without_emoticons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[as, a, woman, you, should, not, complain, abo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[boy, dats, cold, tyga, dwn, bad, for, cuffin,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[dawg, you, ever, fuck, a, bitch, and, she, st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[she, look, like, a, tranny]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[the, shit, you, hear, about, me, might, be, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>[yous, a, muthaf, in, lie, right, his, tl, is,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>[you, have, gone, and, broke, the, wrong, hear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>[young, buck, wanna, eat, dat, nigguh, like, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>[youu, got, wild, bitches, tellin, you, lies]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         preprocessing_without_emoticons\n",
              "0      [as, a, woman, you, should, not, complain, abo...\n",
              "1      [boy, dats, cold, tyga, dwn, bad, for, cuffin,...\n",
              "2      [dawg, you, ever, fuck, a, bitch, and, she, st...\n",
              "3                           [she, look, like, a, tranny]\n",
              "4      [the, shit, you, hear, about, me, might, be, t...\n",
              "...                                                  ...\n",
              "24778  [yous, a, muthaf, in, lie, right, his, tl, is,...\n",
              "24779  [you, have, gone, and, broke, the, wrong, hear...\n",
              "24780  [young, buck, wanna, eat, dat, nigguh, like, i...\n",
              "24781      [youu, got, wild, bitches, tellin, you, lies]\n",
              "24782  [ruffled, ntac, eileen, dahlia, beautiful, col...\n",
              "\n",
              "[24783 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2myUpc0qvrJ0"
      },
      "source": [
        "We include an additional column with emoticons and the '!' since we have seen that it is given importance in Vader's sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bL0yXSiLUNpr",
        "outputId": "e3e70c87-448e-4c81-910e-e96e17e94af8"
      },
      "source": [
        "###### EXECUTING PREPROCESSING WITH EMOTICONS ######\n",
        "\n",
        "pattern = \"\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~‚Äú‚Äù‚Ä¶¬ª‚Äô\"\n",
        "p.set_options(p.OPT.URL,p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.SMILEY, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED, p.OPT.NUMBER)\n",
        "\n",
        "# Each sentence of tweet\n",
        "sentences = []\n",
        "for tweet in df['clean_tweet']:\n",
        "    result = tknzr.tokenize(tweet)\n",
        "    list_token = []\n",
        "    for word in result:\n",
        "      # Change emoticon for text\n",
        "      token = p.clean(word)\n",
        "      token = replaceUnknownEmoticon(token.encode('unicode-escape').decode('ASCII'))\n",
        "\n",
        "      if token == \"\":\n",
        "        pass\n",
        "      else: token = token.encode('ASCII').decode('unicode-escape')\n",
        "\n",
        "      # Remove the capital letters, if the word is not capitalized entirely.\n",
        "      if not token.isupper():\n",
        "        token = token.lower()\n",
        "      if token.isupper() and len(token)==1:\n",
        "        token = token.lower()\n",
        "\n",
        "      if is_remove_contractions(token): # Remove contractions\n",
        "        token = remove_contractions(token)\n",
        "        token = token.split(\" \")\n",
        "        list_token.append(token[0])\n",
        "        list_token.append(token[1])\n",
        "      else: # Clean unuseful data (\",_,...)\n",
        "        token = cleanUnusefulData(token)\n",
        "        if(token is not \"\"):\n",
        "          list_token.append(token)\n",
        "    \n",
        "    sentences.append(list_token)\n",
        "\n",
        "df_result['preprocessing_with_emoticons'] = sentences\n",
        "df_result"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessing_without_emoticons</th>\n",
              "      <th>preprocessing_with_emoticons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[as, a, woman, you, should, not, complain, abo...</td>\n",
              "      <td>[as, a, woman, you, should, not, complain, abo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[boy, dats, cold, tyga, dwn, bad, for, cuffin,...</td>\n",
              "      <td>[boy, dats, cold, tyga, dwn, bad, for, cuffin,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[dawg, you, ever, fuck, a, bitch, and, she, st...</td>\n",
              "      <td>[dawg, !, !, !, you, ever, fuck, a, bitch, and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[she, look, like, a, tranny]</td>\n",
              "      <td>[she, look, like, a, tranny]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[the, shit, you, hear, about, me, might, be, t...</td>\n",
              "      <td>[the, shit, you, hear, about, me, might, be, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>[yous, a, muthaf, in, lie, right, his, tl, is,...</td>\n",
              "      <td>[yous, a, muthaf, in, lie, right, !, his, TL, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>[you, have, gone, and, broke, the, wrong, hear...</td>\n",
              "      <td>[you, have, gone, and, broke, the, wrong, hear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>[young, buck, wanna, eat, dat, nigguh, like, i...</td>\n",
              "      <td>[young, buck, wanna, eat, !, !, dat, nigguh, l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>[youu, got, wild, bitches, tellin, you, lies]</td>\n",
              "      <td>[youu, got, wild, bitches, tellin, you, lies]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
              "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         preprocessing_without_emoticons                       preprocessing_with_emoticons\n",
              "0      [as, a, woman, you, should, not, complain, abo...  [as, a, woman, you, should, not, complain, abo...\n",
              "1      [boy, dats, cold, tyga, dwn, bad, for, cuffin,...  [boy, dats, cold, tyga, dwn, bad, for, cuffin,...\n",
              "2      [dawg, you, ever, fuck, a, bitch, and, she, st...  [dawg, !, !, !, you, ever, fuck, a, bitch, and...\n",
              "3                           [she, look, like, a, tranny]                       [she, look, like, a, tranny]\n",
              "4      [the, shit, you, hear, about, me, might, be, t...  [the, shit, you, hear, about, me, might, be, t...\n",
              "...                                                  ...                                                ...\n",
              "24778  [yous, a, muthaf, in, lie, right, his, tl, is,...  [yous, a, muthaf, in, lie, right, !, his, TL, ...\n",
              "24779  [you, have, gone, and, broke, the, wrong, hear...  [you, have, gone, and, broke, the, wrong, hear...\n",
              "24780  [young, buck, wanna, eat, dat, nigguh, like, i...  [young, buck, wanna, eat, !, !, dat, nigguh, l...\n",
              "24781      [youu, got, wild, bitches, tellin, you, lies]      [youu, got, wild, bitches, tellin, you, lies]\n",
              "24782  [ruffled, ntac, eileen, dahlia, beautiful, col...  [ruffled, ntac, eileen, dahlia, beautiful, col...\n",
              "\n",
              "[24783 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF_y0149mnHV"
      },
      "source": [
        "## Correct Wrong Words\r\n",
        "\r\n",
        "With the following functions it is possible to implement a word corrector, which will use the big.txt file to correct the words of the tweets. Therefore, the big.txt file needs to be loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eApcrBROZn_"
      },
      "source": [
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmhjtJ-A-eIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c044865-e1f5-49d2-df2d-de79bc9d4dd0"
      },
      "source": [
        "def is_emoji(s):\n",
        "    count = 0\n",
        "    for emoji in UNICODE_EMOJI:\n",
        "        count += s.count(emoji)\n",
        "        if count > 1:\n",
        "            return False\n",
        "    return bool(count)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPYVGm9QDn8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36096a08-73c0-4d0d-b43f-d6643e476c07"
      },
      "source": [
        "bad_words = []\n",
        "with open('bad_words.txt') as f:\n",
        "    for linea in f:\n",
        "      bad_words.append(linea[:len(linea)-1])\n",
        "print(bad_words)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2g1c', '2 girls 1 cup', 'acrotomophilia', 'alabama hot pocket', 'alaskan pipeline', 'anal', 'anilingus', 'anus', 'apeshit', 'arsehole', 'ass', 'asshole', 'assmunch', 'auto erotic', 'autoerotic', 'babeland', 'baby batter', 'baby juice', 'ball gag', 'ball gravy', 'ball kicking', 'ball licking', 'ball sack', 'ball sucking', 'bangbros', 'bangbus', 'bareback', 'barely legal', 'barenaked', 'bastard', 'bastardo', 'bastinado', 'bbw', 'bdsm', 'beaner', 'beaners', 'beaver cleaver', 'beaver lips', 'beastiality', 'bestiality', 'big black', 'big breasts', 'big knockers', 'big tits', 'bimbos', 'birdlock', 'bitch', 'bitches', 'black cock', 'blonde action', 'blonde on blonde action', 'blowjob', 'blow job', 'blow your load', 'blue waffle', 'blumpkin', 'bollocks', 'bondage', 'boner', 'boob', 'boobs', 'booty call', 'brown showers', 'brunette action', 'bukkake', 'bulldyke', 'bullet vibe', 'bullshit', 'bung hole', 'bunghole', 'busty', 'butt', 'buttcheeks', 'butthole', 'camel toe', 'camgirl', 'camslut', 'camwhore', 'carpet muncher', 'carpetmuncher', 'chocolate rosebuds', 'cialis', 'circlejerk', 'cleveland steamer', 'clit', 'clitoris', 'clover clamps', 'clusterfuck', 'cock', 'cocks', 'coprolagnia', 'coprophilia', 'cornhole', 'coon', 'coons', 'creampie', 'cum', 'cumming', 'cumshot', 'cumshots', 'cunnilingus', 'cunt', 'darkie', 'date rape', 'daterape', 'deep throat', 'deepthroat', 'dendrophilia', 'dick', 'dildo', 'dingleberry', 'dingleberries', 'dirty pillows', 'dirty sanchez', 'doggie style', 'doggiestyle', 'doggy style', 'doggystyle', 'dog style', 'dolcett', 'domination', 'dominatrix', 'dommes', 'donkey punch', 'double dong', 'double penetration', 'dp action', 'dry hump', 'dvda', 'eat my ass', 'ecchi', 'ejaculation', 'erotic', 'erotism', 'escort', 'eunuch', 'fag', 'faggot', 'fecal', 'felch', 'fellatio', 'feltch', 'female squirting', 'femdom', 'figging', 'fingerbang', 'fingering', 'fisting', 'foot fetish', 'footjob', 'frotting', 'fuck', 'fuck buttons', 'fuckin', 'fucking', 'fucktards', 'fudge packer', 'fudgepacker', 'futanari', 'gangbang', 'gang bang', 'gay sex', 'genitals', 'giant cock', 'girl on', 'girl on top', 'girls gone wild', 'goatcx', 'goatse', 'god damn', 'gokkun', 'golden shower', 'goodpoop', 'goo girl', 'goregasm', 'grope', 'group sex', 'g-spot', 'guro', 'hand job', 'handjob', 'hard core', 'hardcore', 'hentai', 'homoerotic', 'honkey', 'hooker', 'horny', 'hot carl', 'hot chick', 'how to kill', 'how to murder', 'huge fat', 'humping', 'incest', 'intercourse', 'jack off', 'jail bait', 'jailbait', 'jelly donut', 'jerk off', 'jigaboo', 'jiggaboo', 'jiggerboo', 'jizz', 'juggs', 'kike', 'kinbaku', 'kinkster', 'kinky', 'knobbing', 'leather restraint', 'leather straight jacket', 'lemon party', 'livesex', 'lolita', 'lovemaking', 'make me come', 'male squirting', 'masturbate', 'masturbating', 'masturbation', 'menage a trois', 'milf', 'missionary position', 'mong', 'motherfucker', 'mound of venus', 'mr hands', 'muff diver', 'muffdiving', 'nambla', 'nawashi', 'negro', 'neonazi', 'nigga', 'nigger', 'nig nog', 'nimphomania', 'nipple', 'nipples', 'nsfw', 'nsfw images', 'nude', 'nudity', 'nutten', 'nympho', 'nymphomania', 'octopussy', 'omorashi', 'one cup two girls', 'one guy one jar', 'orgasm', 'orgy', 'paedophile', 'paki', 'panties', 'panty', 'pedobear', 'pedophile', 'pegging', 'penis', 'phone sex', 'piece of shit', 'pikey', 'pissing', 'piss pig', 'pisspig', 'playboy', 'pleasure chest', 'pole smoker', 'ponyplay', 'poof', 'poon', 'poontang', 'punany', 'poop chute', 'poopchute', 'porn', 'porno', 'pornography', 'prince albert piercing', 'pthc', 'pubes', 'pussy', 'queaf', 'queef', 'quim', 'raghead', 'raging boner', 'rape', 'raping', 'rapist', 'rectum', 'reverse cowgirl', 'rimjob', 'rimming', 'rosy palm', 'rosy palm and her 5 sisters', 'rusty trombone', 'sadism', 'santorum', 'scat', 'schlong', 'scissoring', 'semen', 'sex', 'sexcam', 'sexo', 'sexy', 'sexual', 'sexually', 'sexuality', 'shaved beaver', 'shaved pussy', 'shemale', 'shibari', 'shit', 'shitblimp', 'shitty', 'shota', 'shrimping', 'skeet', 'slanteye', 'slut', 's&m', 'smut', 'snatch', 'snowballing', 'sodomize', 'sodomy', 'spastic', 'spic', 'splooge', 'splooge moose', 'spooge', 'spread legs', 'spunk', 'strap on', 'strapon', 'strappado', 'strip club', 'style doggy', 'suck', 'sucks', 'suicide girls', 'sultry women', 'swastika', 'swinger', 'tainted love', 'taste my', 'tea bagging', 'threesome', 'throating', 'thumbzilla', 'tied up', 'tight white', 'tit', 'tits', 'titties', 'titty', 'tongue in a', 'topless', 'tosser', 'towelhead', 'tranny', 'tribadism', 'tub girl', 'tubgirl', 'tushy', 'twat', 'twink', 'twinkie', 'two girls one cup', 'undressing', 'upskirt', 'urethra play', 'urophilia', 'vagina', 'venus mound', 'viagra', 'vibrator', 'violet wand', 'vorarephilia', 'voyeur', 'voyeurweb', 'voyuer', 'vulva', 'wank', 'wetback', 'wet dream', 'white power', 'whore', 'worldsex', 'wrapping men', 'wrinkled starfish', 'xx', 'xxx', 'yaoi', 'yellow showers', 'yiffy', 'zoophilia', 'üñï']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdMw6OHGwQGD"
      },
      "source": [
        "‚ö†Ô∏è‚ö†Ô∏è **¬°¬°¬° NO EXECUTE !!!** ‚ö†Ô∏è‚ö†Ô∏è\r\n",
        "\r\n",
        "Using the following piece of code, we correct all the words in the tweets. However, the execution time is approximately 30 minutes. \r\n",
        "\r\n",
        "Therefore, we have provided the resulting CSV separately. You can run it in the following part of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BolJxLejKgS-"
      },
      "source": [
        "df_correct_words = pd.DataFrame(columns=['preprocessing_with_emoticons','preprocessing_without_emoticons'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QWxXroVpTJX"
      },
      "source": [
        "########## ‚ö†Ô∏è HIGH EXECUTION TIME ‚ö†Ô∏è ##########\r\n",
        "\r\n",
        "listTweetsWithout = []\r\n",
        "for i in range(0,len(df_result['preprocessing_without_emoticons'])):\r\n",
        "  listTweet = []\r\n",
        "  for j in range(0,len(df_result['preprocessing_without_emoticons'][i])):\r\n",
        "    if df_result['preprocessing_without_emoticons'][i][j] not in bad_words:\r\n",
        "      listTweet.append(correction(str(df_result['preprocessing_without_emoticons'][i][j])))\r\n",
        "    else:\r\n",
        "      listTweet.append(df_result['preprocessing_without_emoticons'][i][j])\r\n",
        "  listTweetsWithout.append(listTweet)\r\n",
        "\r\n",
        "df_correct_words['preprocessing_without_emoticons'] = listTweetsWithout"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5LXU9uZKkOb"
      },
      "source": [
        "listTweetsWith = []\n",
        "for i in range(0,len(df_result['preprocessing_with_emoticons'])):\n",
        "  listTweet = []\n",
        "  for j in range(0,len(df_result['preprocessing_with_emoticons'][i])):\n",
        "    if df_result['preprocessing_with_emoticons'][i][j] not in bad_words:\n",
        "      if is_emoji(str(df_result['preprocessing_with_emoticons'][i][j])) or df_result['preprocessing_with_emoticons'][i][j]=='!':\n",
        "        listTweet.append(str(df_result['preprocessing_with_emoticons'][i][j]))\n",
        "      else:\n",
        "        listTweet.append(correction(str(df_result['preprocessing_with_emoticons'][i][j])))\n",
        "    else:\n",
        "      listTweet.append(df_result['preprocessing_with_emoticons'][i][j])\n",
        "  listTweetsWith.append(listTweet)\n",
        "\n",
        "\n",
        "df_correct_words['preprocessing_with_emoticons'] = listTweetsWith\n",
        "\n",
        "\n",
        "#############################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agrpdeRxKVpx"
      },
      "source": [
        "df_correct_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cgk0NiaKpE5"
      },
      "source": [
        "df_correct_words.to_csv('df_correct_words.csv')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crP8yKzBb_Sf"
      },
      "source": [
        "## Stop words\r\n",
        "\r\n",
        "A stop word is a word in common use, which does not provide information. It does not influence when expressing feelings of hatred. Therefore, we eliminate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "bDa7pArf-yj8",
        "outputId": "e41fe535-cf46-45d2-d001-fcb071113328"
      },
      "source": [
        "english_stops = set(stopwords.words('english'))\n",
        "clear_sent_emoticon = []\n",
        "clear_sent_not_emoticon = []\n",
        "for tweet in range(0,len(df_result['preprocessing_with_emoticons'])):\n",
        "  clear_sent_emoticon.append([word for word in df_correct_words['preprocessing_with_emoticons'][tweet] if word not in english_stops]) \n",
        "  clear_sent_not_emoticon.append([word for word in df_correct_words['preprocessing_without_emoticons'][tweet] if word not in english_stops]) \n",
        "\n",
        "df_result['preprocessing_with_emoticons'] = clear_sent_emoticon\n",
        "df_result['preprocessing_without_emoticons'] = clear_sent_not_emoticon\n",
        "df_result"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessing_without_emoticons</th>\n",
              "      <th>preprocessing_with_emoticons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[woman, complain, cleaning, house, man, always...</td>\n",
              "      <td>[woman, complain, cleaning, house, man, always...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[boy, days, cold, tea, bad, coffin, dat, st, p...</td>\n",
              "      <td>[boy, days, cold, tea, bad, coffin, dat, st, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[dawn, ever, fuck, bitch, start, cry, confused...</td>\n",
              "      <td>[dawn, !, !, !, ever, fuck, bitch, start, cry,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[look, like, tranny]</td>\n",
              "      <td>[look, like, tranny]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[shit, hear, might, true, might, baker, bitch,...</td>\n",
              "      <td>[shit, hear, might, true, might, baker, bitch,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>[mutual, lie, right, trash, mine, bible, scrip...</td>\n",
              "      <td>[mutual, lie, right, !, trash, mine, bible, sc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>[gone, broke, wrong, heart, baby, drove, redne...</td>\n",
              "      <td>[gone, broke, wrong, heart, baby, drove, redne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>[young, buck, anna, eat, dat, nigh, like, aunt...</td>\n",
              "      <td>[young, buck, anna, eat, !, !, dat, nigh, like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>[got, wild, bitches, telling, lies]</td>\n",
              "      <td>[got, wild, bitches, telling, lies]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>[ruffled, near, eleven, dahlia, beautiful, col...</td>\n",
              "      <td>[ruffled, near, eleven, dahlia, beautiful, col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         preprocessing_without_emoticons                       preprocessing_with_emoticons\n",
              "0      [woman, complain, cleaning, house, man, always...  [woman, complain, cleaning, house, man, always...\n",
              "1      [boy, days, cold, tea, bad, coffin, dat, st, p...  [boy, days, cold, tea, bad, coffin, dat, st, p...\n",
              "2      [dawn, ever, fuck, bitch, start, cry, confused...  [dawn, !, !, !, ever, fuck, bitch, start, cry,...\n",
              "3                                   [look, like, tranny]                               [look, like, tranny]\n",
              "4      [shit, hear, might, true, might, baker, bitch,...  [shit, hear, might, true, might, baker, bitch,...\n",
              "...                                                  ...                                                ...\n",
              "24778  [mutual, lie, right, trash, mine, bible, scrip...  [mutual, lie, right, !, trash, mine, bible, sc...\n",
              "24779  [gone, broke, wrong, heart, baby, drove, redne...  [gone, broke, wrong, heart, baby, drove, redne...\n",
              "24780  [young, buck, anna, eat, dat, nigh, like, aunt...  [young, buck, anna, eat, !, !, dat, nigh, like...\n",
              "24781                [got, wild, bitches, telling, lies]                [got, wild, bitches, telling, lies]\n",
              "24782  [ruffled, near, eleven, dahlia, beautiful, col...  [ruffled, near, eleven, dahlia, beautiful, col...\n",
              "\n",
              "[24783 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0yT0SulgVli"
      },
      "source": [
        "## Lemmatize all terms\r\n",
        "\r\n",
        "With lemmatization we group the inflected forms of the different words so that they can be analyzed as a single element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "0kMOZqUvlZ_N",
        "outputId": "788be63d-467e-438d-9fb7-eb2c30567b54"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "clear_sent_emoticon = []\n",
        "clear_sent_not_emoticon = []\n",
        "for tweet in range(0,len(df_result['preprocessing_with_emoticons'])):\n",
        "  clear_sent_emoticon.append([lemmatizer.lemmatize(word) for word in df_correct_words['preprocessing_with_emoticons'][tweet]])\n",
        "  clear_sent_not_emoticon.append([lemmatizer.lemmatize(word) for word in df_correct_words['preprocessing_without_emoticons'][tweet]])\n",
        "\n",
        "df_result['preprocessing_with_emoticons'] = clear_sent_emoticon\n",
        "df_result['preprocessing_without_emoticons'] = clear_sent_not_emoticon\n",
        "df_result"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessing_without_emoticons</th>\n",
              "      <th>preprocessing_with_emoticons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[a, a, woman, you, should, not, complain, abou...</td>\n",
              "      <td>[a, a, woman, you, should, not, complain, abou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[boy, day, cold, tea, down, bad, for, coffin, ...</td>\n",
              "      <td>[boy, day, cold, tea, down, bad, for, coffin, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[dawn, you, ever, fuck, a, bitch, and, she, st...</td>\n",
              "      <td>[dawn, !, !, !, you, ever, fuck, a, bitch, and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[she, look, like, a, tranny]</td>\n",
              "      <td>[she, look, like, a, tranny]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[the, shit, you, hear, about, me, might, be, t...</td>\n",
              "      <td>[the, shit, you, hear, about, me, might, be, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>[you, a, mutual, in, lie, right, his, to, is, ...</td>\n",
              "      <td>[you, a, mutual, in, lie, right, !, his, of, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>[you, have, gone, and, broke, the, wrong, hear...</td>\n",
              "      <td>[you, have, gone, and, broke, the, wrong, hear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>[young, buck, anna, eat, dat, nigh, like, i, a...</td>\n",
              "      <td>[young, buck, anna, eat, !, !, dat, nigh, like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>[you, got, wild, bitch, telling, you, lie]</td>\n",
              "      <td>[you, got, wild, bitch, telling, you, lie]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>[ruffled, near, eleven, dahlia, beautiful, col...</td>\n",
              "      <td>[ruffled, near, eleven, dahlia, beautiful, col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         preprocessing_without_emoticons                       preprocessing_with_emoticons\n",
              "0      [a, a, woman, you, should, not, complain, abou...  [a, a, woman, you, should, not, complain, abou...\n",
              "1      [boy, day, cold, tea, down, bad, for, coffin, ...  [boy, day, cold, tea, down, bad, for, coffin, ...\n",
              "2      [dawn, you, ever, fuck, a, bitch, and, she, st...  [dawn, !, !, !, you, ever, fuck, a, bitch, and...\n",
              "3                           [she, look, like, a, tranny]                       [she, look, like, a, tranny]\n",
              "4      [the, shit, you, hear, about, me, might, be, t...  [the, shit, you, hear, about, me, might, be, t...\n",
              "...                                                  ...                                                ...\n",
              "24778  [you, a, mutual, in, lie, right, his, to, is, ...  [you, a, mutual, in, lie, right, !, his, of, i...\n",
              "24779  [you, have, gone, and, broke, the, wrong, hear...  [you, have, gone, and, broke, the, wrong, hear...\n",
              "24780  [young, buck, anna, eat, dat, nigh, like, i, a...  [young, buck, anna, eat, !, !, dat, nigh, like...\n",
              "24781         [you, got, wild, bitch, telling, you, lie]         [you, got, wild, bitch, telling, you, lie]\n",
              "24782  [ruffled, near, eleven, dahlia, beautiful, col...  [ruffled, near, eleven, dahlia, beautiful, col...\n",
              "\n",
              "[24783 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMGC-RzQwbQY"
      },
      "source": [
        "Finally we save the preprocessed information in a csv to work more comfortably in another notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pVXkV1OZSqX"
      },
      "source": [
        "df_result['class'] = df['class']\n",
        "df_result.to_csv(\"prepocessed_data.csv\")"
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}