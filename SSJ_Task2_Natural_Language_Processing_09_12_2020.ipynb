{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSJ_Task2_Natural_Language_Processing_09-12-2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiobm3/ESI_MachineLearning/blob/NLP/SSJ_Task2_Natural_Language_Processing_09_12_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIKabJAeqFMt"
      },
      "source": [
        "\n",
        "\n",
        "> We are the SSJ team and this is our work about natural language processing.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmNVisjhv2oV"
      },
      "source": [
        "#-*- coding: UTF-8 -*-\n",
        "import io\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import re"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-uS5d_dJZE"
      },
      "source": [
        "## 0. Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nox0V2_q2Vuw"
      },
      "source": [
        "We start by loading the data file that contains the tweets with which the study will work. The file format to upload is CSV.\n",
        "\n",
        "For this purpose, we use some commands provided by Google Colab itself to access files on our computer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7muMGF_CQve",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "87f2c931-e130-4c72-fc4a-fb1dbbf37cab"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')),sep=',')\n",
        "\n",
        "df"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da4acb3a-62de-401e-b249-2e5c36ea6416\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da4acb3a-62de-401e-b249-2e5c36ea6416\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving labeled_data.csv to labeled_data (3).csv\n",
            "User uploaded file \"labeled_data.csv\" with length 2546446 bytes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>25291</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>25292</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>25294</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>25295</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>youu got wild bitches tellin you lies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>25296</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                              tweet\n",
              "0               0  ...  !!! RT @mayasolovely: As a woman you shouldn't...\n",
              "1               1  ...  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
              "2               2  ...  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
              "3               3  ...  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
              "4               4  ...  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
              "...           ...  ...                                                ...\n",
              "24778       25291  ...  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
              "24779       25292  ...  you've gone and broke the wrong heart baby, an...\n",
              "24780       25294  ...  young buck wanna eat!!.. dat nigguh like I ain...\n",
              "24781       25295  ...              youu got wild bitches tellin you lies\n",
              "24782       25296  ...  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
              "\n",
              "[24783 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUYtzK1fykzw"
      },
      "source": [
        "3 classes: 0 --> hate speech, 1 --> offensive language, 2 --> neither"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkiEwhzbelrP"
      },
      "source": [
        "# 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upQTMV99GXCZ"
      },
      "source": [
        "##Remove unuseful data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojY36_VFHlH3",
        "outputId": "ccfafdb6-7e05-4e61-ff97-e54ff4d95227"
      },
      "source": [
        "pattern = \"\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~”“…\" # Es la variable string.puntuation sin el caracter ! porque lo vemos importante \n",
        "def cleanUnusefulData(sentence):\n",
        "  sentence = sentence.translate(str.maketrans('', '', pattern))\n",
        "  return sentence\n",
        "def removeUnusefulExclamation(sentence):\n",
        "  if sentence[0] == '!':\n",
        "    sentence = removeUnusefulExclamation(sentence[1:])\n",
        "  else:\n",
        "    return sentence\n",
        "  return sentence\n",
        "\n",
        "print(cleanUnusefulData(\"!!!!!!!!!!!!! RT @ShenikaRoberts: The shit yo*\"))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!!!!!!!!!!!!!RTShenikaRobertsTheshityo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4h67o8aGWtU"
      },
      "source": [
        "for t in range(0,len(df)):\n",
        "  tweet = df.iloc[t]['tweet']\n",
        "  tweet = removeUnusefulExclamation(tweet)\n",
        "  df.loc[t,'tweet'] = tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJJYq7h4au0L"
      },
      "source": [
        "##Replace emoticons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frvfJW0NtnNl"
      },
      "source": [
        "\n",
        "Lectura de fichero emoticonos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbx33glitktw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e6524d-ec13-4098-f63e-ec6d085291bb"
      },
      "source": [
        "dict_emoticons = {}\n",
        "with open('emoticons.txt') as f:\n",
        "    for linea in f:\n",
        "      info = linea.split(\",\")\n",
        "      #emoticon_code = \"&#\"+str(info[1])+\";\"\n",
        "      emoticon_code = info[0]\n",
        "      emoticon_word = info[2]\n",
        "      dict_emoticons[emoticon_code] = emoticon_word[:len(emoticon_word)-1]\n",
        "print(dict_emoticons)\n",
        "\n",
        "def replaceEmoticon(word):\n",
        "  #p = re.compile(\"&#x[0-9A-Z]+;\")\n",
        "  #s = re.compile(\"&#128073;&#128076\") #sex\n",
        "  #m = p.findall(word)\n",
        "  #print(word)\n",
        "  if word in dict_emoticons:\n",
        "    word = word.replace(str(word),dict_emoticons[word])\n",
        "  return word\n",
        "\n",
        "#def replaceEmoticon2(word):\n",
        "#  if word in dict_emoticons: ## Si es un emoticono que está en el diccionario.\n",
        "#    #word = word.replace(str(word),dict_emoticons[word]) ## Si está en el diccionario, como queremos mantenerla, devolvemos lo mismo.\n",
        "#    return word\n",
        "#  else:\n",
        "#    patron = re.compile('&#[0-9]+')\n",
        "#    if patron.match(word) is None: ## Si no es un emoticono\n",
        "#      return word\n",
        "#    else: ## Si es un emoticono no definido en el diccionario. AHORA SI, DEVOLVEMOS CADENA VACÍA.\n",
        "#      word = word.replace(str(word),\"\")\n",
        "#      return word"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'😀': 'smile', '😁': 'smile', '😂': 'laugh', '😃': 'smile', '😄': 'smile', '😆': 'smile', '😉': 'wink', '😊': 'smile', '😒': 'unamused', '😕': 'confused', '😗': 'kiss', '😘': 'kiss', '😙': 'kiss', '😚': 'kiss', '😞': 'dissapointed', '😟': 'worried', '😠': 'angry', '😡': 'angry', '😢': 'sad', '😨': 'frightened', '😪': 'sleepy', '😫': 'tired', '😭': 'sad', '😮': 'surprised', '😯': 'surprised', '😱': 'frightened', '😲': 'astonished', '😳': 'flushed', '😴': 'sleepy', '😵': 'confused', '😶': 'quiet', '🤐': 'quiet', '🤒': 'ill', '🤔': 'thoughtful', '🤡': 'clown', '🤢': 'sucks', '🤣': 'laugh', '\\U0001f92c': 'angry', '\\U0001f92e': 'sucks', '\\U0001f92f': 'astonished'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzmvKruZiOc"
      },
      "source": [
        "##Remove contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG2J7JPHZqIO"
      },
      "source": [
        "dict_contractions = {'aren\\'t':'are not', 'can\\'t':'can not', 'couldn\\'t':'could not', 'didn\\'t':'did not', 'don\\'t':'do not', 'doesn\\'t':'does not', 'hadn\\'t':'had not',\n",
        "                       'haven\\'t':'have not', 'he\\'s':'he is', 'he\\'ll':'he will', 'he\\'d':'he would', 'here\\'s':'here is', 'i\\'m':'i am', 'i\\'ve':'i have', 'i\\'ll':'i will',\n",
        "                       'i\\'d':'i would', 'isn\\'t':'is not','it\\'s':'it is', 'it\\'ll':'it will', 'mustn\\'t':'must not', 'she\\'s':'she is', 'she\\'ll':'she will', 'she\\'d':'she would',\n",
        "                       'shouldn\\'t':'should not', 'that\\'s':'that is', 'there\\'s':'there is', 'they\\'re':'they are', 'they\\'ve':'they have', 'they\\'ll':'they will', 'they\\'d':'they would',\n",
        "                       'they\\'d':'they had', 'wasn\\'t':'was not', 'we\\'re':'we are', 'we\\'re':'we are', 'we\\'ve':'we have', 'we\\'ll':'we will', 'we\\'d':'we would', 'weren\\'t':'were not', 'what\\'s':'what is',\n",
        "                       'where\\'s':'where is', 'who\\'s':'who is', 'who\\'ll':'who will', 'won\\'t':'will not', 'wouldn\\'t':'would not', 'you\\'re': 'you are', 'you\\'ve': 'you have', \n",
        "                       'you\\'ll':'you will', 'you\\'d':'you would', 'y\\'all': 'you all', 'could\\'ve': 'could have', 'hasn\\'t': 'has not', 'let\\'s': 'let us'}\n",
        "\n",
        "def remove_contractions(word):\n",
        "  return word.replace(word,dict_contractions[word])\n",
        "  \n",
        "def is_remove_contractions(word):\n",
        "  if word in dict_contractions:\n",
        "    return True    \n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJz6UfZ_iZQ_"
      },
      "source": [
        "Now the emoticons that we have decided has been changed for text representing the meaning of the emoticon.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBPUWLdKSckv"
      },
      "source": [
        "## Executing preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNDh43SCZeG2"
      },
      "source": [
        "# We create a new dataframe to save result in different columns\n",
        "df_result = pd.DataFrame()"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7cE8FWj3SQX"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "\n",
        "import preprocessor as p\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "p.set_options(p.OPT.EMOJI, p.OPT.URL,p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.SMILEY, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED, p.OPT.NUMBER)\n",
        "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "# each sentence of tweet\n",
        "sentences = []\n",
        "for tweet in df['tweet']:\n",
        "    word_repeat= \"\"\n",
        "    result = tknzr.tokenize(tweet)\n",
        "    list_token = []\n",
        "    for word in result:\n",
        "      \n",
        "      if word_repeat != word:\n",
        "\n",
        "        # Cambiar emoticono por texto\n",
        "        token = replaceEmoticon(word)\n",
        "        \n",
        "        # limpiar hastag y menciones\n",
        "        token = p.clean(token)\n",
        "\n",
        "        # Cambiar primera letra mayuscula solo si la siguiente es minuscula\n",
        "        if not token.isupper():\n",
        "          token = token.lower()\n",
        "        if token.isupper() and len(token)==1:\n",
        "          token = token.lower()\n",
        "\n",
        "        # Cambiar contraciones (you're) por palabra entera\n",
        "        if is_remove_contractions(token):\n",
        "          token = remove_contractions(token)\n",
        "          token = token.split(\" \")\n",
        "          list_token.append(token[0])\n",
        "          list_token.append(token[1])\n",
        "        else:\n",
        "          # limpiar información inútil (\",_,...)\n",
        "          token = cleanUnusefulData(token)\n",
        "          if(token is not \"\"):\n",
        "            list_token.append(token)\n",
        "        word_repeat = word\n",
        "      \n",
        "    sentences.append(list_token)\n",
        "  #print(result)\n",
        "\n",
        "df_result['preprocessing_without_emoticons'] = sentences\n",
        "df_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL0yXSiLUNpr"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "\n",
        "import preprocessor as p\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "p.set_options(p.OPT.URL,p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.SMILEY, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED, p.OPT.NUMBER)\n",
        "\n",
        "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "# each sentence of tweet\n",
        "sentences = []\n",
        "for tweet in df['tweet']:\n",
        "    word_repeat= \"\"\n",
        "    result = tknzr.tokenize(tweet)\n",
        "    list_token = []\n",
        "    for word in result:\n",
        "      \n",
        "      if word_repeat != word:\n",
        "\n",
        "        # limpiar hastag y menciones\n",
        "        token = p.clean(word)\n",
        "\n",
        "        # Cambiar primera letra mayuscula solo si la siguiente es minuscula\n",
        "        if not token.isupper():\n",
        "          token = token.lower()\n",
        "        if token.isupper() and len(token)==1:\n",
        "          token = token.lower()\n",
        "\n",
        "        # Cambiar contraciones (you're) por palabra entera\n",
        "        if is_remove_contractions(token):\n",
        "          token = remove_contractions(token)\n",
        "          token = token.split(\" \")\n",
        "          list_token.append(token[0])\n",
        "          list_token.append(token[1])\n",
        "        else:\n",
        "          # limpiar información inútil (\",_,...)\n",
        "          token = cleanUnusefulData(token)\n",
        "          if(token is not \"\"):\n",
        "            list_token.append(token)\n",
        "        word_repeat = word\n",
        "    \n",
        "    sentences.append(list_token)\n",
        "  #print(result)\n",
        "\n",
        "df_result['preprocessing_with_emoticons'] = sentences\n",
        "df_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM3-R6n4a0fq",
        "outputId": "ad9053ac-e861-4f36-cb96-ed8e8852c3ea"
      },
      "source": [
        "#Comprobacion emoticonos\n",
        "print(df.iloc[24778]['tweet'])\n",
        "print(df_result['preprocessing_with_emoticons'][24778])\n",
        "print(df_result['preprocessing_without_emoticons'][24778])\n",
        "\n",
        "print(df.iloc[5]['tweet'])\n",
        "print(df_result['preprocessing_with_emoticons'][5])\n",
        "print(df_result['preprocessing_without_emoticons'][5])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you's a muthaf***in lie &#8220;@LifeAsKing: @20_Pearls @corey_emanuel right! His TL is trash &#8230;. Now, mine? Bible scriptures and hymns&#8221;\n",
            "['yous', 'a', 'muthaf', 'in', 'lie', 'right', '!', 'his', 'TL', 'is', 'trash', 'now', 'mine', 'bible', 'scriptures', 'and', 'hymns']\n",
            "['yous', 'a', 'muthaf', 'in', 'lie', 'right', '!', 'his', 'TL', 'is', 'trash', 'now', 'mine', 'bible', 'scriptures', 'and', 'hymns']\n",
            "!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just blows me..claim you so faithful and down for somebody but still fucking with hoes! &#128514;&#128514;&#128514;\"\n",
            "['!', 'the', 'shit', 'just', 'blows', 'me', 'claim', 'you', 'so', 'faithful', 'and', 'down', 'for', 'somebody', 'but', 'still', 'fucking', 'with', 'hoes', '!', '😂']\n",
            "['!', 'the', 'shit', 'just', 'blows', 'me', 'claim', 'you', 'so', 'faithful', 'and', 'down', 'for', 'somebody', 'but', 'still', 'fucking', 'with', 'hoes', '!', 'laugh']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZaH72DOB0iT"
      },
      "source": [
        "for sentence in range(0,len(df_result)):\r\n",
        "  print(df_result['preprocessing_with_emoticons'][sentence])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF_y0149mnHV"
      },
      "source": [
        " [los metodos estan explicados en esta pagina web - How to Write a Spelling Corrector](https://norvig.com/spell-correct.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eApcrBROZn_"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def words(text):\n",
        "  aux =\"\"\n",
        "  aux = re.findall(r'\\w+', text)\n",
        "  return aux\n",
        "\n",
        "WORDS_COUNTER = []\n",
        "for sentence in range(0,len(df_result)):\n",
        "  WORDS = []\n",
        "  cell = df_result.iloc[sentence]['preprocessing_without_emoticons']\n",
        "  for i in cell:\n",
        "    w = words(i)\n",
        "    WORDS_COUNTER += w\n",
        "    if w:\n",
        "      WORDS.append(w[0])\n",
        "  df_result.loc[sentence]['preprocessing_without_emoticons'] = WORDS\n",
        "    \n",
        "WORDS_COUNTER = Counter(WORDS_COUNTER)\n",
        "\n",
        "def P(word, N=sum(WORDS_COUNTER.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS_COUNTER[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS_COUNTER)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pdkY3IPq5he",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "12740976-165a-4ad4-eb44-320aadd2fae7"
      },
      "source": [
        "correction('yous')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yous'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC7wGSYrx9Yv"
      },
      "source": [
        "Cantidad de palabras diferentes y numero de palabras totales. Además se muestra el TOP 10 de la palabra más usada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjqT2Olo6EqS"
      },
      "source": [
        "print(\"We can see that there are \"+ str(len(WORDS_COUNTER))+ \" distinct words, which together appear \"+ str(sum(WORDS_COUNTER.values())) +\" times.\")\n",
        "print(\"Words more used: \")\n",
        "WORDS_COUNTER.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbT3SW1mxwen"
      },
      "source": [
        "Se han almacenado todas las palabras existentes y ha contado cuantas aparecen repetidas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j94xKHBkT-CB"
      },
      "source": [
        "WORDS_COUNTER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crP8yKzBb_Sf"
      },
      "source": [
        "##Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDa7pArf-yj8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "ec047f98-33d2-48d2-8595-766e6967da71"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stops = set(stopwords.words('english'))\n",
        "clear_sent_emoticon = []\n",
        "clear_sent_not_emoticon = []\n",
        "for tweet in range(0,len(df_result['preprocessing_with_emoticons'])):\n",
        "  clear_sent_emoticon.append([word for word in df_result['preprocessing_with_emoticons'][tweet] if word not in english_stops]) \n",
        "  clear_sent_not_emoticon.append([word for word in df_result['preprocessing_without_emoticons'][tweet] if word not in english_stops]) \n",
        "\n",
        "df_result['preprocessing_with_emoticons'] = clear_sent_emoticon\n",
        "df_result['preprocessing_without_emoticons'] = clear_sent_not_emoticon\n",
        "df_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessing_without_emoticons</th>\n",
              "      <th>preprocessing_with_emoticons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[woman, complain, cleaning, house, man, always...</td>\n",
              "      <td>[woman, complain, cleaning, house, man, always...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[boy, dats, cold, tyga, dwn, bad, cuffin, dat,...</td>\n",
              "      <td>[boy, dats, cold, tyga, dwn, bad, cuffin, dat,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[dawg, ever, fuck, bitch, start, cry, confused...</td>\n",
              "      <td>[dawg, !, ever, fuck, bitch, start, cry, confu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[look, like, tranny]</td>\n",
              "      <td>[look, like, tranny]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[shit, hear, might, true, might, faker, bitch,...</td>\n",
              "      <td>[shit, hear, might, true, might, faker, bitch,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>[yous, muthaf, lie, right, TL, trash, mine, bi...</td>\n",
              "      <td>[yous, muthaf, lie, “, right, !, TL, trash, …,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>[gone, broke, wrong, heart, baby, drove, redne...</td>\n",
              "      <td>[gone, broke, wrong, heart, baby, drove, redne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>[young, buck, wanna, eat, dat, nigguh, like, a...</td>\n",
              "      <td>[young, buck, wanna, eat, !, dat, nigguh, like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>[youu, got, wild, bitches, tellin, lies]</td>\n",
              "      <td>[youu, got, wild, bitches, tellin, lies]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
              "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         preprocessing_without_emoticons                       preprocessing_with_emoticons\n",
              "0      [woman, complain, cleaning, house, man, always...  [woman, complain, cleaning, house, man, always...\n",
              "1      [boy, dats, cold, tyga, dwn, bad, cuffin, dat,...  [boy, dats, cold, tyga, dwn, bad, cuffin, dat,...\n",
              "2      [dawg, ever, fuck, bitch, start, cry, confused...  [dawg, !, ever, fuck, bitch, start, cry, confu...\n",
              "3                                   [look, like, tranny]                               [look, like, tranny]\n",
              "4      [shit, hear, might, true, might, faker, bitch,...  [shit, hear, might, true, might, faker, bitch,...\n",
              "...                                                  ...                                                ...\n",
              "24778  [yous, muthaf, lie, right, TL, trash, mine, bi...  [yous, muthaf, lie, “, right, !, TL, trash, …,...\n",
              "24779  [gone, broke, wrong, heart, baby, drove, redne...  [gone, broke, wrong, heart, baby, drove, redne...\n",
              "24780  [young, buck, wanna, eat, dat, nigguh, like, a...  [young, buck, wanna, eat, !, dat, nigguh, like...\n",
              "24781           [youu, got, wild, bitches, tellin, lies]           [youu, got, wild, bitches, tellin, lies]\n",
              "24782  [ruffled, ntac, eileen, dahlia, beautiful, col...  [ruffled, ntac, eileen, dahlia, beautiful, col...\n",
              "\n",
              "[24783 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0yT0SulgVli"
      },
      "source": [
        "## Lemmatize all terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kMOZqUvlZ_N"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "clear_sent_emoticon = []\n",
        "clear_sent_not_emoticon = []\n",
        "for tweet in range(0,len(df_result['preprocessing_with_emoticons'])):\n",
        "  clear_sent_emoticon.append([lemmatizer.lemmatize(word) for word in df_result['preprocessing_with_emoticons'][tweet]])\n",
        "  clear_sent_not_emoticon.append([lemmatizer.lemmatize(word) for word in df_result['preprocessing_without_emoticons'][tweet]])\n",
        "\n",
        "df_result['preprocessing_with_emoticons'] = clear_sent_emoticon\n",
        "df_result['preprocessing_without_emoticons'] = clear_sent_not_emoticon\n",
        "df_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgzs8N1rF3A_"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdUCApcRvo6a"
      },
      "source": [
        "## Destokenizador\n",
        "\n",
        "To do the vectorization, we have chosen to put back all the tokens that make a sentence. We think that this will make the development of this process easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Qh7RLCvn8w"
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "tweets_detokenize = []\n",
        "\n",
        "for i in range(0,len(df_result)):\n",
        "  tweets_detokenize.append( TreebankWordDetokenizer().detokenize(df_result.iloc[i]['preprocessing_without_emoticons']))\n",
        "\n",
        "print(len(tweets_detokenize))\n",
        "tweets_detokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XgcqsoG6ofl"
      },
      "source": [
        "df_result['detokenization'] = tweets_detokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y-g6JsMCrmzP",
        "outputId": "4119089f-4e7e-4a0d-a227-781e77b21b62"
      },
      "source": [
        "df_result.iloc[0]['detokenization']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'woman complain cleaning house man always take trash'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GVpS8gYsH7K"
      },
      "source": [
        "## TFIDF\n",
        "\n",
        "In order to start using TFIDF, we will first have to create a CountVectorizer to count the number of words (term frequency)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZSByAr09npl"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=1)\n",
        "\n",
        "X = vectorizer.fit_transform(tweets_detokenize)\n",
        "\n",
        "#X_tokens = countvectorizer.get_features_names()\n",
        "\n",
        "print(X.shape)\n",
        "print(X.toarray())\n",
        "print(vectorizer.vocabulary_)\n",
        "print(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4rjX-ypv4Y-"
      },
      "source": [
        "Cosas que poner antes del codigo que he encontrado en el interne\n",
        "\n",
        "With Tfidfvectorizer you compute the word counts, idf and tf-idf values all at once. It’s really simple.\n",
        "\n",
        "With Tfidfvectorizer on the contrary, you will do all three steps at once. Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_8QwPhVv-_c"
      },
      "source": [
        "# TF-IDF\n",
        "# https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/\n",
        "# https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a\n",
        "# https://es.ryte.com/wiki/TF*IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=1)\n",
        "\n",
        "X = vectorizer.fit_transform(tweets_detokenize)\n",
        "\n",
        "tfidf = X.toarray()\n",
        "\n",
        "print(X.shape)\n",
        "print(tfidf)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(X[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMVV65cHJmh8"
      },
      "source": [
        "listaTFIDF = []\r\n",
        "\r\n",
        "for i in range(0,X.shape[0]):\r\n",
        "  listaTFIDF.append(X[i])\r\n",
        "\r\n",
        "df_vectorization = pd.DataFrame()\r\n",
        "\r\n",
        "df_vectorization['TFIDF'] = listaTFIDF\r\n",
        "\r\n",
        "hola = df_vectorization.iloc[7]['TFIDF'] # PRIMER TWEET\r\n",
        "\r\n",
        "#print(hola[0].get_shape())\r\n",
        "\r\n",
        "print(hola.getnnz(axis=0))\r\n",
        "\r\n",
        "#print(hola[0][16878])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S79rQ91nBa8g"
      },
      "source": [
        "## TFIDF + N-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A17uKu4mFnBm"
      },
      "source": [
        "df_result.to_csv('df_result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9172MN6LhUAD"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
        "\"\"\"f=open(\"kk.txt\",\"w\")\n",
        "f.write(\"machacando\\n\")\n",
        "f.close()\"\"\"\n",
        "\n",
        "X = vectorizer.fit_transform(tweets_detokenize)\n",
        "\n",
        "tfidf = X.toarray()\n",
        "print(tfidf)\n",
        "print(vectorizer.vocabulary_)\n",
        "print (X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBw3Q0uDw135"
      },
      "source": [
        "## TFIDF + N-grams + POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3qeqgIGMUv"
      },
      "source": [
        "# Part of Speech Tagging\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "listaTweets = []\n",
        "contador = 0\n",
        "for tweet in df_result['preprocessing_without_emoticons']:\n",
        "  tags = []\n",
        "  #if '' in tweet:\n",
        "   # tweet.remove('')\n",
        "  #for word in tweet: \n",
        "  #tags.append(nltk.pos_tag(word))\n",
        "  print(tweet)\n",
        "  listaTweets.append(nltk.pos_tag(tweet))\n",
        "  contador = contador + 1\n",
        "  print(contador)\n",
        "    #listaTweets.append(tags)\n",
        "  \n",
        "#df_result = listaTweets\n",
        "df_result['POS'] = listaTweets\n",
        "df_result['POS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9QopHCiMc83"
      },
      "source": [
        "# filter nouns\n",
        "nouns = []\n",
        "for tweet in df_result['POS']:\n",
        "    nouns.append([word[0] for word in tweet if word[1] == 'NN'])\n",
        "nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMaQkneZxAB-"
      },
      "source": [
        "## TFIDF + N-grams + POS tagging + Other features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZLdOQhFcOvK"
      },
      "source": [
        "### Number  of  words\r\n",
        "\r\n",
        "Para garantizar que estamos contando las palabras de los tweets sin preprocesar, las contamos de df, no de df_result. Lo mismo para el siguiente apartado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KqnLVj-cQqG"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "listNumberWords2 = []\r\n",
        "\r\n",
        "#df['tweet'][0]\r\n",
        "\r\n",
        "for i in range(0,len(df['tweet'])):\r\n",
        "  #words = df['tweet'][i].split() # OPCIÓN SEVILLA (NO RECOMENDADA)\r\n",
        "  words = word_tokenize(df['tweet'][i]) ### ASÍ LO HACE EN EL CUADERNO EL TEACHER\r\n",
        "  num_words = len(words)\r\n",
        "  listNumberWords.append(num_words)\r\n",
        "  #print(num_words)\r\n",
        "\r\n",
        "df_result['number_of_words'] = listNumberWords\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdIVJa5WcXN7"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "\r\n",
        "listNumberSentences = []\r\n",
        "\r\n",
        "for i in range(0,len(df['tweet'])):\r\n",
        "  #sentences = df['tweet'][i].split('.') # OPCIÓN SEVILLA (NO RECOMENDADA)\r\n",
        "  sentences = sent_tokenize(df['tweet'][i]) ### ASÍ LO HACE EN EL CUADERNO EL TEACHER\r\n",
        "  num_sentences = len(sentences)\r\n",
        "  listNumberSentences.append(num_sentences)\r\n",
        "  #print(num_sentences)\r\n",
        "\r\n",
        "df_result['number_of_sentences'] = listNumberSentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZHRJCC-k_vOH",
        "outputId": "48939573-a536-4c3b-ab2a-e01c6ebed101"
      },
      "source": [
        "df['tweet'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lJ8lnraEjdSO",
        "outputId": "d1095f8b-6b59-4907-c73b-cac0382c2f5f"
      },
      "source": [
        "df_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessing_without_emoticons</th>\n",
              "      <th>preprocessing_with_emoticons</th>\n",
              "      <th>number_of_words</th>\n",
              "      <th>number_of_sentences</th>\n",
              "      <th>number_of_sentences2</th>\n",
              "      <th>number_of_words2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[as, a, woman, you, should, not, complain, abo...</td>\n",
              "      <td>[as, a, woman, you, should, not, complain, abo...</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[boy, dats, cold, tyga, dwn, bad, for, cuffin,...</td>\n",
              "      <td>[boy, dats, cold, tyga, dwn, bad, for, cuffin,...</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[dawg, !, you, ever, fuck, a, bitch, and, she,...</td>\n",
              "      <td>[dawg, !, you, ever, fuck, a, bitch, and, she,...</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[she, look, like, a, tranny]</td>\n",
              "      <td>[she, look, like, a, tranny]</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[the, shit, you, hear, about, me, might, be, t...</td>\n",
              "      <td>[the, shit, you, hear, about, me, might, be, t...</td>\n",
              "      <td>25</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>[yous, a, muthaf, in, lie, right, !, his, TL, ...</td>\n",
              "      <td>[yous, a, muthaf, in, lie, “, right, !, his, T...</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>[you, have, gone, and, broke, the, wrong, hear...</td>\n",
              "      <td>[you, have, gone, and, broke, the, wrong, hear...</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>[young, buck, wanna, eat, !, dat, nigguh, like...</td>\n",
              "      <td>[young, buck, wanna, eat, !, dat, nigguh, like...</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>[youu, got, wild, bitches, tellin, you, lies]</td>\n",
              "      <td>[youu, got, wild, bitches, tellin, you, lies]</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
              "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         preprocessing_without_emoticons  ... number_of_words2\n",
              "0      [as, a, woman, you, should, not, complain, abo...  ...               34\n",
              "1      [boy, dats, cold, tyga, dwn, bad, for, cuffin,...  ...               26\n",
              "2      [dawg, !, you, ever, fuck, a, bitch, and, she,...  ...               35\n",
              "3                           [she, look, like, a, tranny]  ...               20\n",
              "4      [the, shit, you, hear, about, me, might, be, t...  ...               43\n",
              "...                                                  ...  ...              ...\n",
              "24778  [yous, a, muthaf, in, lie, right, !, his, TL, ...  ...               39\n",
              "24779  [you, have, gone, and, broke, the, wrong, hear...  ...               15\n",
              "24780  [young, buck, wanna, eat, !, dat, nigguh, like...  ...               17\n",
              "24781      [youu, got, wild, bitches, tellin, you, lies]  ...                7\n",
              "24782  [ruffled, ntac, eileen, dahlia, beautiful, col...  ...               25\n",
              "\n",
              "[24783 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXJ0H30rbmUk"
      },
      "source": [
        "### Hatred n-gram dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9JyS6CLlnPZ"
      },
      "source": [
        "hatredDF = pd.read_csv('refined_ngram_dict.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "B48PiT94mpQN",
        "outputId": "f0cf8b3a-fcca-4442-d2cf-9b2ab992243f"
      },
      "source": [
        "hatredDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ngram</th>\n",
              "      <th>prophate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allah akbar</td>\n",
              "      <td>0.870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>blacks</td>\n",
              "      <td>0.583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chink</td>\n",
              "      <td>0.467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chinks</td>\n",
              "      <td>0.542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dykes</td>\n",
              "      <td>0.602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>nigga you a lame</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>niggers are in my</td>\n",
              "      <td>0.714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>wit a lame nigga</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>you a lame bitch</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>you fuck wit a</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ngram  prophate\n",
              "0          allah akbar     0.870\n",
              "1               blacks     0.583\n",
              "2                chink     0.467\n",
              "3               chinks     0.542\n",
              "4                dykes     0.602\n",
              "..                 ...       ...\n",
              "173   nigga you a lame     0.556\n",
              "174  niggers are in my     0.714\n",
              "175   wit a lame nigga     0.556\n",
              "176   you a lame bitch     0.556\n",
              "177     you fuck wit a     0.556\n",
              "\n",
              "[178 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLaOGq2xnbM8"
      },
      "source": [
        "values = []\n",
        "for x in range(0,len(df_result)):\n",
        "  sentence = df_result.iloc[x]['detokenization']\n",
        "  n = 0\n",
        "  value = 0\n",
        "  \n",
        "  for i in range(0,len(hatredDF)):\n",
        "    pattern = hatredDF.iloc[i]['ngram']\n",
        "    result = re.match(pattern,sentence)\n",
        "    if result is not None:\n",
        "      n += 1\n",
        "      value += float(hatredDF.iloc[i]['prophate'])\n",
        "  if n != 0:\n",
        "    value = value / n\n",
        "  values.append(value)\n",
        "  print(\"{} and {}\".format(value,len(values)))\n",
        "\n",
        "df_result['hatred_ngrams'] = values\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx6cUzm7DVTC"
      },
      "source": [
        "###Sentiment analisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoRmzK3kDN6I"
      },
      "source": [
        "https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BBmKRofxEjP"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sentiment_analyzer  = SentimentIntensityAnalyzer() \n",
        "sentence = 'hey you fucking bastard'\n",
        "sentiment = sentiment_analyzer.polarity_scores(sentence)\n",
        "sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPYAVNtuik5E"
      },
      "source": [
        "# RESTO DE COSAS QUE HEMOS PEGAO\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWd6hPbDlq0b"
      },
      "source": [
        "# Vectorization\n",
        "# Based on counting\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=1)\n",
        "\n",
        "X = vectorizer.fit_transform(tweets_detokenize)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
        "\n",
        "X = vectorizer.fit_transform(tweets_detokenize)\n",
        "\n",
        "tfidf = X.toarray()\n",
        "print(tfidf)\n",
        "print(vectorizer.vocabulary_)\n",
        "print (X.shape)\n",
        "\n",
        "pos_vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
        "\n",
        "#length\n",
        "len(pos_vocab)\n",
        "\n",
        "#pos_vocab\n",
        "#vectorizer.get_feature_names()\n",
        "\n",
        "# KEYwORDS\n",
        "#import numpy\n",
        "\n",
        "#keywords = [vectorizer.get_feature_names()[numpy.argmax(x)] for x in X.toarray()]\n",
        "# keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8YN4fcXNE9-"
      },
      "source": [
        "## Chunker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "DjMf13D0NGV-",
        "outputId": "01eed5ba-9536-4292-c93f-d2e6ddca284b"
      },
      "source": [
        "!pip install svgling\n",
        "import svgling\n",
        "\n",
        "#errors are due to tkinter because it is trying to display the tree . # No funciona porque detecta simbolos !?!-~#@ Cuando no halla funcionara bien\n",
        "'''\n",
        "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "chunkParser = nltk.RegexpParser(chunkGram)\n",
        "for tweet in df['POS']:\n",
        "  #print(tweet)\n",
        "  result = chunkParser.parse(tweet)\n",
        "  #svgling.draw_tree(result)\n",
        "'''\n",
        "\n",
        "#sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
        "sentence = df['POS'][900]\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(sentence)\n",
        "svgling.draw_tree(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: svgling in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.6/dist-packages (from svgling) (1.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TreeLayout(Tree('S', [(',', ','), (',', ','), (',', ','), ('|', 'NNP'), Tree('NP', [('|', 'JJ'), ('german', 'JJ'), ('milf', 'NN')]), ('getting', 'VBG'), Tree('NP', [('young', 'JJ'), ('old', 'JJ'), ('cock', 'NN')]), Tree('NP', [('pussy', 'NN')])]))"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,528.0,168.0\" width=\"528px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"4.54545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.27273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.54545%\" x=\"4.54545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.81818%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.54545%\" x=\"9.09091%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.57576%\" x=\"13.6364%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">|</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.4242%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"27.2727%\" x=\"21.2121%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"22.2222%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">|</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.1111%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"44.4444%\" x=\"22.2222%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">german</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.4444%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">milf</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.8485%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.6364%\" x=\"48.4848%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">getting</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.303%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"27.2727%\" x=\"62.1212%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"38.8889%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">young</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.4444%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"27.7778%\" x=\"38.8889%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">old</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.7778%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cock</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75.7576%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.6061%\" x=\"89.3939%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pussy</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.697%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    }
  ]
}